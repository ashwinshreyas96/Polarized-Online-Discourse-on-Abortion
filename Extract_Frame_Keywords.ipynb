{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sage\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=''\n",
    "filespath='./data/Baseline'\n",
    "files=os.listdir(filespath)\n",
    "for file in files:\n",
    "    with open(os.path.join(filespath,file),'r') as myfile:\n",
    "        baseline=baseline+' '+myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "def lemmatize(sent):\n",
    "    s=[token.lemma_ for token in nlp(sent)]\n",
    "    s=' '.join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=baseline.split('\\n')\n",
    "baseline_arr=[b.strip() for b in baseline_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_chars=\"~?!./\\:;+=&^%$#@(,)[]_*\"\n",
    "emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"u\"\\U0001F300-\\U0001F5FF\"u\"\\U0001F680-\\U0001F6FF\" u\"\\U0001F1E0-\\U0001F1FF\"\"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "def deep_clean(x):\n",
    "    x=x.lower()\n",
    "    x=re.sub(r'http\\S+', '', x)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    remove_chars = str.maketrans('', '', irrelevant_chars)\n",
    "    x = x.translate(remove_digits)\n",
    "    x = x.translate(remove_chars)\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    x=x.replace('!','')\n",
    "    x=x.replace('?','')\n",
    "    x=x.replace('@','')\n",
    "    x=x.replace('&','')\n",
    "    x=x.replace('$','')\n",
    "    x=x.replace('``','')\n",
    "    x=x.replace(\"'s\",'')\n",
    "    x=x.replace(\"''\",'')\n",
    "    x=[t for t in x.split() if len(t)>3]\n",
    "    x=' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=[deep_clean(b) for b in baseline_arr]\n",
    "base_words=[]\n",
    "for b in tqdm(baseline_arr):\n",
    "    b_arr=b.split()\n",
    "    b_arr=[b for b in b_arr if b not in stp]\n",
    "    base_words.extend(b_arr)\n",
    "base_count=Counter(base_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scores(eta,K=100):\n",
    "    scores=eta[(-eta).argsort()[:K]]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go over files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:24<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "filespath='./data/Wiki_Data'\n",
    "files=os.listdir(filespath)\n",
    "words_dict={}\n",
    "for file in tqdm(files):\n",
    "    category_arr = open(os.path.join(filespath,file),'r').readlines()\n",
    "    #category_arr=[lemmatize(t) for t in category]\n",
    "    category_arr=[deep_clean(t.strip()) for t in category_arr]\n",
    "    \n",
    "    category_words=[]\n",
    "    for b in category_arr:\n",
    "        b_arr=b.split()\n",
    "        b_arr=[b for b in b_arr if b not in stp]\n",
    "        category_words.extend(b_arr)\n",
    "    category_count=Counter(category_words)\n",
    "    \n",
    "    vocab = [word for word,count in Counter(category_count).most_common(5000)]\n",
    "    x_terr = np.array([category_count[word] for word in vocab])\n",
    "    x_base = np.array([base_count[word] for word in vocab]) + 1.\n",
    "    \n",
    "    mu = np.log(x_base) - np.log(x_base.sum())\n",
    "\n",
    "    eta = sage.estimate(x_terr,mu)\n",
    "\n",
    "    category=sage.topK(eta,vocab,K=200)\n",
    "    scores=ret_scores(eta,200)\n",
    "    category_dict={}\n",
    "    for i in range(len(category)):\n",
    "        category_dict[category[i]]=scores[i]\n",
    "    words_dict[file]=category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in words_dict:\n",
    "    name=file.split('.txt')[0]+'.csv'\n",
    "    df=pd.DataFrame(words_dict[file].items(),columns=['word','relevance_score'])\n",
    "    df.to_csv('./data/Unigrams/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=''\n",
    "filespath='./data/Baseline'\n",
    "files=os.listdir(filespath)\n",
    "for file in files:\n",
    "    with open(os.path.join(filespath,file),'r') as myfile:\n",
    "        baseline=baseline+' '+myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=baseline.split('\\n')\n",
    "baseline_arr=[b.strip() for b in baseline_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "def deep_clean(x):\n",
    "    x=x.lower()\n",
    "    x=re.sub(r'http\\S+', '', x)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    remove_chars = str.maketrans('', '', irrelevant_chars)\n",
    "    x = x.translate(remove_digits)\n",
    "    x = x.translate(remove_chars)\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    x=x.replace('!','')\n",
    "    x=x.replace('?','')\n",
    "    x=x.replace('@','')\n",
    "    x=x.replace('&','')\n",
    "    x=x.replace('$','')\n",
    "    x=x.replace('``','')\n",
    "    x=x.replace(\"'s\",'')\n",
    "    x=x.replace(\"''\",'')\n",
    "    x=[t for t in x.split() if len(t)>3]\n",
    "    x=' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=[deep_clean(b) for b in baseline_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31928/31928 [00:05<00:00, 6040.98it/s]\n"
     ]
    }
   ],
   "source": [
    "base_words=[]\n",
    "for b in tqdm(baseline_arr):\n",
    "    nltk_tokens = word_tokenize(b)\n",
    "\n",
    "    b_arr =list(ngrams(nltk_tokens,2))\n",
    "    #b_arr=b.split()\n",
    "    \n",
    "    #b_arr=[b for b in b_arr if b not in stp]\n",
    "    base_words.extend(b_arr)\n",
    "base_count=Counter(base_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scores(eta,K=100):\n",
    "    scores=eta[(-eta).argsort()[:K]]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:43<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "filespath='./data/Wiki_Data'\n",
    "files=os.listdir(filespath)\n",
    "words_dict={}\n",
    "for file in tqdm(files):\n",
    "    category_arr = open(os.path.join(filespath,file),'r').readlines()\n",
    "    #category_arr=[lemmatize(t) for t in category]\n",
    "    category_arr=[deep_clean(t.strip()) for t in category_arr]\n",
    "    \n",
    "    category_words=[]\n",
    "    for b in category_arr:\n",
    "        nltk_tokens = word_tokenize(b)\n",
    "        b_arr =list(ngrams(nltk_tokens,2))\n",
    "        #b_arr=b.split()\n",
    "        #b_arr=[b for b in b_arr if b not in stp]\n",
    "        category_words.extend(b_arr)\n",
    "    category_count=Counter(category_words)\n",
    "    \n",
    "    vocab = [word for word,count in Counter(category_count).most_common(5000)]\n",
    "    x_terr = np.array([category_count[word] for word in vocab])\n",
    "    x_base = np.array([base_count[word] for word in vocab]) + 1.\n",
    "    \n",
    "    mu = np.log(x_base) - np.log(x_base.sum())\n",
    "\n",
    "    eta = sage.estimate(x_terr,mu)\n",
    "\n",
    "    category=sage.topK(eta,vocab,K=50)\n",
    "    scores=ret_scores(eta,50)\n",
    "    category_dict={}\n",
    "    for i in range(len(category)):\n",
    "        category_dict[category[i]]=scores[i]\n",
    "    words_dict[file]=category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('justice', 'blackmun'): 3.293121249813784,\n",
       " ('pregnant', 'woman'): 2.5158605681127444,\n",
       " ('trimester', 'framework'): 2.38936337505552,\n",
       " ('right', 'abortion'): 2.278412952937506,\n",
       " ('texas', 'abortion'): 2.2435983122229923,\n",
       " ('partialbirth', 'abortion'): 2.1157230212844906,\n",
       " ('sarah', 'weddington'): 2.071126993773123,\n",
       " ('abortion', 'would'): 2.071126993773123,\n",
       " ('unborn', 'child'): 2.071126993773123,\n",
       " ('population', 'control'): 1.919850716888008,\n",
       " ('justice', 'thomas'): 1.8585558650284635,\n",
       " ('norma', 'mccorvey'): 1.8585558650284635,\n",
       " ('abortion', 'decision'): 1.6721995484541272,\n",
       " ('illegal', 'abortion'): 1.5760298529888002,\n",
       " ('abortions', 'were'): 1.5760298529888002,\n",
       " ('about', 'abortion'): 1.5760298529888002,\n",
       " ('court', 'abortion'): 1.5760298529888002,\n",
       " ('texas', 'heartbeat'): 1.5760298529888002,\n",
       " ('prequickening', 'abortions'): 1.5760298529888002,\n",
       " ('roe', \"''\"): 1.5760298529888002,\n",
       " ('abortion', 'which'): 1.5760298529888002,\n",
       " ('abortion', 'united'): 1.5760298529888002,\n",
       " ('prenatal', 'life'): 1.5760298529888002,\n",
       " ('that', 'fetus'): 1.5760298529888002,\n",
       " ('said', 'abortion'): 1.5760298529888002,\n",
       " ('thought', 'would'): 1.5760298529888002,\n",
       " ('first', 'trimester'): 1.5172712164860591,\n",
       " ('abortion', 'should'): 1.4987883622524172,\n",
       " ('should', 'legal'): 1.3782692245479786,\n",
       " ('justice', 'rehnquist'): 1.3275767530875044,\n",
       " ('support', 'abortion'): 1.3275767530875044,\n",
       " ('banning', 'abortion'): 1.0934423129792021,\n",
       " (\"''\", 'abortion'): 1.0934423129792021,\n",
       " ('after', 'weeks'): 1.0934423129792021,\n",
       " ('weeks', 'pregnancy'): 1.0934423129792021,\n",
       " ('rape', 'incest'): 1.0934423129792021,\n",
       " ('reproductive', 'justice'): 1.0934423129792021,\n",
       " ('concurring', 'opinions'): 1.0934423129792021,\n",
       " ('affirmative', 'right'): 1.0934423129792021,\n",
       " ('march', 'life'): 1.0934423129792021,\n",
       " ('life', 'health'): 1.0934423129792021,\n",
       " ('abortions', 'except'): 1.0934423129792021,\n",
       " ('medical', 'technology'): 1.0934423129792021,\n",
       " ('potential', 'life'): 1.0934423129792021,\n",
       " ('choose', 'have'): 1.0934423129792021,\n",
       " ('ruling', 'about'): 1.0934423129792021,\n",
       " ('child', \"''\"): 1.0934423129792021,\n",
       " ('abortion', 'statutes'): 1.0934423129792021,\n",
       " ('decision', \"''\"): 1.0934423129792021,\n",
       " (\"''\", 'predicted'): 1.0934423129792021}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict[files[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in words_dict:\n",
    "    name=file.split('.txt')[0]+'.csv'\n",
    "    df=pd.DataFrame(words_dict[file].items(),columns=['word','relevance_score'])\n",
    "    df.to_csv('./data/Bigrams/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline=''\n",
    "filespath='./data/Baseline'\n",
    "files=os.listdir(filespath)\n",
    "for file in files:\n",
    "    with open(os.path.join(filespath,file),'r') as myfile:\n",
    "        baseline=baseline+' '+myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=baseline.split('\\n')\n",
    "baseline_arr=[b.strip() for b in baseline_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "def deep_clean(x):\n",
    "    x=x.lower()\n",
    "    x=re.sub(r'http\\S+', '', x)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    remove_chars = str.maketrans('', '', irrelevant_chars)\n",
    "    x = x.translate(remove_digits)\n",
    "    x = x.translate(remove_chars)\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    x=x.replace('!','')\n",
    "    x=x.replace('?','')\n",
    "    x=x.replace('@','')\n",
    "    x=x.replace('&','')\n",
    "    x=x.replace('$','')\n",
    "    x=x.replace('``','')\n",
    "    x=x.replace(\"'s\",'')\n",
    "    x=x.replace(\"''\",'')\n",
    "    x=[t for t in x.split() if len(t)>3]\n",
    "    x=' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_arr=[deep_clean(b) for b in baseline_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31928/31928 [00:05<00:00, 6342.32it/s]\n"
     ]
    }
   ],
   "source": [
    "base_words=[]\n",
    "for b in tqdm(baseline_arr):\n",
    "    nltk_tokens = word_tokenize(b)\n",
    "\n",
    "    b_arr =list(ngrams(nltk_tokens,3))\n",
    "    #b_arr=b.split()\n",
    "    \n",
    "    #b_arr=[b for b in b_arr if b not in stp]\n",
    "    base_words.extend(b_arr)\n",
    "base_count=Counter(base_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scores(eta,K=100):\n",
    "    scores=eta[(-eta).argsort()[:K]]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:21<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "filespath='./data/Wiki_Data'\n",
    "files=os.listdir(filespath)\n",
    "words_dict={}\n",
    "for file in tqdm(files):\n",
    "    category_arr = open(os.path.join(filespath,file),'r').readlines()\n",
    "    #category_arr=[lemmatize(t) for t in category]\n",
    "    category_arr=[deep_clean(t.strip()) for t in category_arr]\n",
    "    \n",
    "    category_words=[]\n",
    "    for b in category_arr:\n",
    "        nltk_tokens = word_tokenize(b)\n",
    "        b_arr =list(ngrams(nltk_tokens,3))\n",
    "        #b_arr=b.split()\n",
    "        #b_arr=[b for b in b_arr if b not in stp]\n",
    "        category_words.extend(b_arr)\n",
    "    category_count=Counter(category_words)\n",
    "    \n",
    "    vocab = [word for word,count in Counter(category_count).most_common(5000)]\n",
    "    x_terr = np.array([category_count[word] for word in vocab])\n",
    "    x_base = np.array([base_count[word] for word in vocab]) + 1.\n",
    "    \n",
    "    mu = np.log(x_base) - np.log(x_base.sum())\n",
    "\n",
    "    eta = sage.estimate(x_terr,mu)\n",
    "\n",
    "    category=sage.topK(eta,vocab,K=50)\n",
    "    scores=ret_scores(eta,50)\n",
    "    category_dict={}\n",
    "    for i in range(len(category)):\n",
    "        category_dict[category[i]]=scores[i]\n",
    "    words_dict[file]=category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in words_dict:\n",
    "    name=file.split('.txt')[0]+'.csv'\n",
    "    df=pd.DataFrame(words_dict[file].items(),columns=['word','relevance_score'])\n",
    "    df.to_csv('./data/Trigrams/'+name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
